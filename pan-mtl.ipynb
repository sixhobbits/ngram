{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# import pipeline\n",
    "# from lxml.etree import tostring\n",
    "# from lxml.builder import E\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading dataset...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"loading dataset...\")\n",
    "df = datasets.load_pan17(\"../data/training/\")\n",
    "corpus = df.corpus\n",
    "corpus['text'] = corpus.text.apply(lambda x: '\\n'.join(x))  # join tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2660ba148fdb6d40d45a2cde1ebc0938</td>\n",
       "      <td>ar</td>\n",
       "      <td>وين راحوا لفتاتك المليانه حب و غيره\\nشو ال غير...</td>\n",
       "      <td>female</td>\n",
       "      <td>levantine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8190b6c35e3bfc0a7a5b606987f12088</td>\n",
       "      <td>ar</td>\n",
       "      <td>أعوذ بكلمات الله التامات من شر ما خلق\\n♻️ http...</td>\n",
       "      <td>male</td>\n",
       "      <td>levantine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beb2267b45907af46c677b57b181d33b</td>\n",
       "      <td>ar</td>\n",
       "      <td>700 مقاتل مغربي مدرب خارج سيطرة الأجهزة الأمني...</td>\n",
       "      <td>male</td>\n",
       "      <td>maghrebi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>febcaf1e3821d7a53cf440d772307d6c</td>\n",
       "      <td>ar</td>\n",
       "      <td>أبرز #تصريحات #النجوم في العام 2016         \\n...</td>\n",
       "      <td>female</td>\n",
       "      <td>levantine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3ba53c8b59303a381e058d11a6678221</td>\n",
       "      <td>ar</td>\n",
       "      <td>اللَّهم يَا خَالق الحُبّ والنَّوَى ..\\nأعْطِ ل...</td>\n",
       "      <td>male</td>\n",
       "      <td>levantine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             author lang  \\\n",
       "0  2660ba148fdb6d40d45a2cde1ebc0938   ar   \n",
       "1  8190b6c35e3bfc0a7a5b606987f12088   ar   \n",
       "2  beb2267b45907af46c677b57b181d33b   ar   \n",
       "3  febcaf1e3821d7a53cf440d772307d6c   ar   \n",
       "4  3ba53c8b59303a381e058d11a6678221   ar   \n",
       "\n",
       "                                                text  gender    variety  \n",
       "0  وين راحوا لفتاتك المليانه حب و غيره\\nشو ال غير...  female  levantine  \n",
       "1  أعوذ بكلمات الله التامات من شر ما خلق\\n♻️ http...    male  levantine  \n",
       "2  700 مقاتل مغربي مدرب خارج سيطرة الأجهزة الأمني...    male   maghrebi  \n",
       "3  أبرز #تصريحات #النجوم في العام 2016         \\n...  female  levantine  \n",
       "4  اللَّهم يَا خَالق الحُبّ والنَّوَى ..\\nأعْطِ ل...    male  levantine  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train-dev-test\n",
    "train_df = corpus[corpus.lang == 'en'].loc[3600:5600]\n",
    "dev_df = corpus[corpus.lang == 'en'].loc[5601:6100]\n",
    "test_df = corpus[corpus.lang == 'en'].loc[6101:6600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i_m = defaultdict(lambda: len(t2i_m)) # main labels\n",
    "t2i_a = defaultdict(lambda: len(t2i_a)) # aux labels\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "\n",
    "def read_dataset(dataframe):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        main_label, aux_label, text = row.gender, row.variety, row.text\n",
    "        yield ([w2i[x] for x in text.split(\" \")], t2i_m[main_label], t2i_a[aux_label])\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(train_df))[:30]\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(dev_df))[:30]\n",
    "test = list(read_dataset(test_df))\n",
    "nwords = len(w2i)\n",
    "ntags1 = len(t2i_m)\n",
    "ntags2 = len(t2i_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=nan, time=4.56s\n"
     ]
    }
   ],
   "source": [
    "# Start DyNet and defin trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE))  # Word embeddings\n",
    "\n",
    "fwdLSTM = dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)  # Forward LSTM\n",
    "bwdLSTM = dy.LSTMBuilder(1, EMB_SIZE, HID_SIZE, model)\n",
    "\n",
    "H_sm_main = model.add_parameters((64, 2 * HID_SIZE))  # Softmax weights\n",
    "O_sm_main = model.add_parameters((1,64))  # Softmax bias\n",
    "\n",
    "H_sm_aux = model.add_parameters((64, 2 * HID_SIZE))  # Softmax weights\n",
    "O_sm_aux = model.add_parameters((ntags2,64))  # Softmax bias\n",
    "\n",
    "\n",
    "# A function to calculate scores for one value\n",
    "def calc_scores(words, main_tag, aux_tag):\n",
    "    dy.renew_cg()\n",
    "    word_embs = [dy.lookup(W_emb, x) for x in words]\n",
    "    fwd_init = fwdLSTM.initial_state()\n",
    "    bwd_init = bwdLSTM.initial_state()\n",
    "\n",
    "    fwd_embs = fwd_init.transduce(word_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(word_embs))\n",
    "    \n",
    "    repr = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) # use last step as representation\n",
    "    \n",
    "    H_m = dy.parameter(H_sm_main)\n",
    "    O_m = dy.parameter(O_sm_main)\n",
    "    \n",
    "    H_a = dy.parameter(H_sm_aux)\n",
    "    O_a = dy.parameter(O_sm_aux)\n",
    "    \n",
    "    \n",
    "    final_m = O_m*dy.tanh(H_m * repr) # MLP for main task\n",
    "    final_a = O_a*dy.tanh(H_a * repr) # MLP for auxiliary task\n",
    "    \n",
    "    main = dy.binary_log_loss(final_m, dy.scalarInput(main_tag)) # gender\n",
    "    aux = dy.pickneglogsoftmax(final_a, aux_tag) #variety\n",
    "    return main, aux\n",
    "\n",
    "\n",
    "for ITER in range(1):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for words, main_tag, aux_tag in train:\n",
    "        loss = sum(calc_scores(words, main_tag, aux_tag))\n",
    "        train_loss += loss.value()\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tag(words, task):\n",
    "    dy.renew_cg()\n",
    "    word_embs = [dy.lookup(W_emb, x) for x in words]\n",
    "    fwd_init = fwdLSTM.initial_state()\n",
    "    bwd_init = bwdLSTM.initial_state()\n",
    "    ## Q2: run the forward pass of the LSTM\n",
    "    fwd_embs = fwd_init.transduce(word_embs)\n",
    "    bwd_embs = bwd_init.transduce(reversed(word_embs))\n",
    "    repr = dy.concatenate([fwd_embs[-1], bwd_embs[-1]]) # use last step as representation\n",
    "    \n",
    "    H_m = dy.parameter(H_sm_main)\n",
    "    O_m = dy.parameter(O_sm_main)\n",
    "    \n",
    "    H_a = dy.parameter(H_sm_aux)\n",
    "    O_a = dy.parameter(O_sm_aux)\n",
    "    \n",
    "    final_m = O_m * dy.tanh(H_m*repr)\n",
    "    main = dy.logistic(final_m)\n",
    "    \n",
    "    final_a = O_a*dy.tanh(H_a * repr) # MLP for auxiliary task\n",
    "    aux = dy.softmax(final_a) #variety\n",
    "    \n",
    "    if task == 'main':\n",
    "        if main.value() > 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif task =='aux':\n",
    "        return np.argmax(aux.npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "iter 0: eval acc=0.5333\n"
     ]
    }
   ],
   "source": [
    "# Eval\n",
    "eval_correct = 0.0\n",
    "for words, main, aux in dev:\n",
    "    scores = predict_tag(words, 'main')\n",
    "    predict = scores\n",
    "    print(predict, main)\n",
    "    if predict == main:\n",
    "        eval_correct += 1\n",
    "print(\"iter %r: eval acc=%.4f\" % (ITER, eval_correct / len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
